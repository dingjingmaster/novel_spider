# 小说爬虫

本框架适合做定点抓取，尤其适用于每天抓取完本小说。`python` 提供了大量的 `http` 请求库 和 `xml` 或 `html` 解析器，所以开发一个爬虫其实并不难，重要的其实是对抓取数据的格式化和保存。所以我想着写一个简单的框架，只限制数据格式，不限制其它流程。

### 依赖安装
    pip install request
    pip install pyquery

### 目前已有爬虫
    http://www.uu234.cc

### 抓取字段
    书名、作者名、分类、连载/完结状态、描述、封面图片、封面图片URL、书籍URL、章节页URL、章节名、章节序、章节URL、章节内容

### 更新字段
    书籍封面页URL、书籍封面、书籍章节页URL、书籍章节内容、书籍章节URL

### 实现思路

- 抓取流程：
    1. 首先检查已抓取的书籍是否有更新，如果有更新则更新书籍信息
    2. 开始新书籍的抓取
    
- 新书籍的抓取流程如下：
    1. 根据设置的 url 生成书籍列表页 url 得到书籍列表页
    2. 解析书籍列表页获得书籍详情页 url
    3. 解析书籍详情页，获取各书籍信息
    4. 保存书籍信息
    5. 下载书籍封面页图片并保存
    6. 请求书籍章节详情页并保存

- 存储思路

    为了容易管理书籍信息，我将书籍存储全部放到 `mysql` 中如果想要添加 `mongodb` 可以先实现 mongodb 的增删改查，再在 `frame/common/novel.py` 中替换
    - 存储表字段具体看 `sql/novel.sql` 即可
    - 小说信息去重的策略是将小说内容页 url 设置为唯一键，章节信息去重也是如此，所以小说去重只是在一个站内去重（确保精修后的书籍不被修改）
    - `lock` 表示上锁，如果书籍信息上锁，那么该书籍的章节内容也不会再被更新，章节内容上锁则只是不更新本章内容

### 项目结构

- frame 文件夹下用来开发爬虫
- sql 创建 mysql 表的脚本
- url 各个网站要抓取的书籍列表页范围
- main.py 创建指定网站的爬虫并开始抓取

### 开发新的爬虫

1. 给要开发的爬虫取个名字，保存到 `frame/common/param.py` 下
2. 在 `frame/novel_parser/` 下创建新的小说解析器并需要继承 `base/parser` 下的 Parser 
3. 在 `frame/spiders` 下创建新的小说爬虫并需要继承 `base/spider` 需要分别实现 `check()` 和 `run()`
4. 分别在 `parser_factory` 和 `spider_factory` 下注册爬虫
5. 在 `main.py` 中把爬虫放到线程池中，运行即可

### 另

小说信息抓取的程序，我写了很多遍，最开始模仿某些框架拆分出了 `调度器`、`持久化器`、`引擎`、`解析器`... 实话说，越写越不喜欢，越写越纠结于模块的划分。

我认为作为定点抓取，`调度器` 没有任何作用，除非要上线了，针对某些热书做优先抓取，但是这个可以在 spider 类中的 `check()` 函数那里进行控制

现在好了，干脆只有 `解析器` 和 `抓取流程` 实现，总体有一个线程池，每个网站开一个线程，至于数据存储时候的同步问题，交给mysql解决

### 还需要改进的地方

1. 把小说信息的保存从 `novel.py` 中分离出来，将小说信息保存作为第二个基类（也就是其它框架的`持久化器`）
2. 可以去除工厂模式创建 `解析器` 和 `爬虫` ，提供一个注册函数，将所有`解析器`和`爬虫`注册保存到两个字典中，在主函数中遍历这两个字典中的一个，依次加入到线程池中





